{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set up our environment.\n",
    "\n",
    "This Jupyter Notebook needs the following packages installed:\n",
    "- [PyTorch](https://pytorch.org/get-started/locally/)\n",
    "- [transformers](https://huggingface.co/docs/transformers/installation)\n",
    "- [datasets](https://huggingface.co/docs/datasets/installation)\n",
    "- [opendelta](https://opendelta.readthedocs.io/en/latest/notes/installation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare environment in Colab\n",
    "!pip install torch\n",
    "!pip install transformers==4.28.0\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install torchtext==0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/Users/rongwang/Desktop/DL project/path/to/cache\n",
      "env: HF_MODULES_CACHE=/Users/rongwang/Desktop/DL project/path/to/cache\n",
      "env: HF_DATASETS_CACHE=/Users/rongwang/Desktop/DL project/path/to/cache\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set cache directory if desired\n",
    "# if you do not set a cache directory then default values are used (usually '~/.cache')\n",
    "import os\n",
    "CACHE_DIR=os.path.abspath(os.path.expanduser('path/to/cache')) # I donot understand this line\n",
    "%set_env TRANSFORMERS_CACHE $CACHE_DIR\n",
    "%set_env HF_MODULES_CACHE $CACHE_DIR\n",
    "%set_env HF_DATASETS_CACHE $CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "## and then move your model and data to the device before you train or eval. Have fun folks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    DefaultDataCollator,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    DistilBertForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# load dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "# The datasets library is a library for loading and preprocessing datasets for machine learning. The Dataset class is a way to handle large datasets in a way that is memory efficient.\n",
    "\n",
    "\n",
    "\n",
    "def read_data(filepath: str):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return Dataset.from_list(list(json.load(f).values()))\n",
    "\n",
    "\n",
    "filepath_1 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/squad_train.json')\n",
    "filepath_2 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/squad_dev.json')\n",
    "filepath_3 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/rc_train.json')\n",
    "filepath_4 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/rc_dev.json')\n",
    "\n",
    "filepath_5 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/rc_test_1.json')\n",
    "\n",
    "filepath_6 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/rc_test_1.json')\n",
    "\n",
    "squad_train = read_data(filepath_1)\n",
    "squad_dev = read_data(filepath_2)\n",
    "rc_train = read_data(filepath_3)\n",
    "rc_dev = read_data(filepath_4)\n",
    "rc_test = read_data(filepath_5)\n",
    "\n",
    "print(squad_train)\n",
    "print(rc_train)\n",
    "print(rc_dev)\n",
    "print(rc_test)\n",
    "print(rc_test[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'context', 'question', 'answers'],\n",
      "    num_rows: 87599\n",
      "})\n",
      "Dataset({\n",
      "    features: ['context', 'question', 'answers', 'id'],\n",
      "    num_rows: 100\n",
      "})\n",
      "Dataset({\n",
      "    features: ['context', 'question', 'answers', 'id'],\n",
      "    num_rows: 100\n",
      "})\n",
      "Dataset({\n",
      "    features: ['context', 'question', 'id'],\n",
      "    num_rows: 177\n",
      "})\n",
      "Dataset({\n",
      "    features: ['context', 'question', 'id'],\n",
      "    num_rows: 1126\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "# load dataset\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# The datasets library is a library for loading and preprocessing datasets for machine learning. The Dataset class is a way to handle large datasets in a way that is memory efficient.\n",
    "    \n",
    "\n",
    "def read_data(filepath: str):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return Dataset.from_list(list(json.load(f).values()))\n",
    "\n",
    "\n",
    "filepath_1 = os.path.abspath('rc_traindev/squad_train.json')\n",
    "filepath_2 = os.path.abspath('rc_traindev/squad_dev.json')\n",
    "filepath_3 = os.path.abspath('rc_traindev/rc_train.json')\n",
    "filepath_4 = os.path.abspath('rc_traindev/rc_dev.json')\n",
    "\n",
    "filepath_5 = os.path.abspath('rc_traindev/rc_test_1.json')\n",
    "filepath_6 = os.path.abspath('rc_traindev/rc_test_2.json')\n",
    "\n",
    "squad_train = read_data(filepath_1)\n",
    "squad_dev = read_data(filepath_2)\n",
    "rc_train = read_data(filepath_3)\n",
    "rc_dev = read_data(filepath_4)\n",
    "rc_test = read_data(filepath_5)\n",
    "rc_final = read_data(filepath_6)\n",
    "\n",
    "print(squad_train)\n",
    "print(rc_train)\n",
    "print(rc_dev)\n",
    "print(rc_test)\n",
    "print(rc_final)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will preprocess the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(dataset: Dataset, tokenizer, max_length: int = None):\n",
    "    \n",
    "    # dataset preprocessing function which can be used with datasets.map\n",
    "    # modified from https://huggingface.co/docs/transformers/tasks/question_answering\n",
    "    \n",
    "    # this function takes examples and extracts span start and span end on the token level as labels,\n",
    "    # contexts are truncated to fit into the model and samples are padded to the max_length,\n",
    "    # and information needed for evaluation is extracted\n",
    "    \n",
    "    def preprocess_function(examples, tokenizer, max_length):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        inputs = tokenizer(\n",
    "            questions,\n",
    "            examples[\"context\"],\n",
    "            max_length=max_length,\n",
    "            truncation=\"only_second\",\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        offset_mapping = inputs[\"offset_mapping\"]\n",
    "        answers = examples[\"answers\"]\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        context_spans = []\n",
    "\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            answer = answers[i]\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "            context_spans.append((context_start, context_end))\n",
    "\n",
    "            # If the answer is not fully inside the context, label it (0, 0)\n",
    "            if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Otherwise it's the start and end token positions\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "        inputs[\"start_positions\"] = start_positions\n",
    "        inputs[\"end_positions\"] = end_positions\n",
    "        inputs[\"context_span\"] = context_spans\n",
    "        return inputs\n",
    "\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        fn_kwargs=dict(tokenizer=tokenizer, max_length=max_length),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be020243e8d04f0db394541736174d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba302f80cfb4c368421bd84daf621b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced244f74a384b4cb9cfa1b9b5dbd969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84683e4bad934c01accd0683e6546e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess data\n",
    "\n",
    "max_length = 500 ## max length of context + question in tokens, maybe it was too expensive for the GPU \n",
    "\n",
    "model_checkpoint =\"deepset/roberta-large-squad2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenized_squad_train = preprocess_dataset(squad_train, tokenizer)\n",
    "\n",
    "tokenized_squad_dev = preprocess_dataset(squad_dev, tokenizer)\n",
    "\n",
    "tokenized_rc_train = preprocess_dataset(rc_train, tokenizer)\n",
    "tokenized_rc_dev = preprocess_dataset(rc_dev, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn(dataset: Dataset, context_spans, offset_mappings):\n",
    "    # set up metric and labels\n",
    "    squad_metric = load_metric(\"squad\")\n",
    "    \n",
    "    #Handle missing keys: If some samples might not have an 'answers' key, you can use the dict.get() method, which returns None if the key is not present in the dictionary.\n",
    "    # Here's how you can modify your code:\n",
    "    references = [{\"answers\": sample.get(\"answers\"), \"id\": sample.get(\"id\")} for sample in dataset]\n",
    "\n",
    "\n",
    "    # this function extracts the span answers and computes the f1 score\n",
    "    # note that we use an approximation for extracting the best span, i.e. we do not consider all possibilities here\n",
    "    def evaluate(predictions: EvalPrediction):\n",
    "        preds_start_probs = predictions.predictions[0]\n",
    "        preds_end_probs = predictions.predictions[1]\n",
    "        preds_start_idx = preds_start_probs.argmax(axis=1)\n",
    "        preds_end_idx = preds_end_probs.argmax(axis=1)\n",
    "        predictions = [\n",
    "            {\n",
    "                \"prediction_text\": context[offset_mapping[pred_start_idx][0] : offset_mapping[pred_end_idx][1] + 1]\n",
    "                if context_start_idx <= pred_start_idx <= context_end_idx\n",
    "                and context_start_idx <= pred_end_idx <= context_end_idx\n",
    "                else \"\",\n",
    "                \"id\": sample_id,\n",
    "            }\n",
    "            for sample_id, context, (\n",
    "                context_start_idx,\n",
    "                context_end_idx,\n",
    "            ), offset_mapping, pred_start_idx, pred_end_idx in zip(\n",
    "                dataset[\"id\"], dataset[\"context\"], context_spans, offset_mappings, preds_start_idx, preds_end_idx\n",
    "            )\n",
    "        ]\n",
    "        return squad_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "## and then move your model and data to the device before you train or eval. Have fun folks!\n",
    "\n",
    "#installing pytorch to run on mac M1, using \"mps\" instead of 'cuda'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/pvlsm7s116zd61_5k5qx9hkr0000gn/T/ipykernel_13856/2530109866.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  squad_metric = load_metric(\"squad\")\n"
     ]
    }
   ],
   "source": [
    "# set up training arguments and pass to trainer, For few-shot learning, I only trained on rc_train and rc_dev, altogether 200 samples, use 5 epochs \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",    # if running on Colab, set this to \"/content/results\"\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=32,\n",
    "    num_train_epochs=5,  # max_steps will override this value\n",
    "    # max_steps=1000,  # comment out if this is not wanted\n",
    "    weight_decay=0.01,\n",
    "    #logging_dir='./logs',\n",
    "    label_names=[\"start_positions\", \"end_positions\"]\n",
    ")\n",
    "\n",
    "# data collator for batching\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# the actual trainer which performs training and evaluation\n",
    "trainer = Trainer(\n",
    "    model= model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_rc_train,\n",
    "    eval_dataset=tokenized_rc_dev,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=get_evaluate_fn(\n",
    "        rc_dev,\n",
    "        tokenized_rc_dev[\"context_span\"],\n",
    "        tokenized_rc_dev[\"offset_mapping\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3b7153fb8042739c633d028de48a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.1210832595825195,\n",
       " 'eval_exact_match': 30.0,\n",
       " 'eval_f1': 34.61904761904761,\n",
       " 'eval_runtime': 123.1073,\n",
       " 'eval_samples_per_second': 0.812,\n",
       " 'eval_steps_per_second': 0.032}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do an initial evaluation\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform training\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_dataset(dataset: Dataset, tokenizer, max_length: int = 500, stride: int =128 ):\n",
    "    \n",
    "\n",
    "    def preprocess_function(examples, tokenizer, max_length):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        inputs = tokenizer(\n",
    "            questions,\n",
    "            examples[\"context\"],\n",
    "            max_length=max_length,\n",
    "            truncation=\"only_second\",\n",
    "            stride=stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        # Create a map from overflowing tokens to the original sample they came from\n",
    "        sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "        inputs[\"example_id\"] = [examples[\"id\"][i] for i in sample_map]\n",
    "\n",
    "        # Modify the offset mapping to only keep offsets for the context\n",
    "        for i in range(len(inputs[\"input_ids\"])):\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "            offset = inputs[\"offset_mapping\"][i]\n",
    "            inputs[\"offset_mapping\"][i] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        fn_kwargs=dict(tokenizer=tokenizer, max_length=max_length),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the following code is for the prediction part for test data, without any answers labels.\n",
    "compared with the prediction part for train and validation data, we don't need to compute the f1 score.\n",
    "but it is more complicated, it chohoses the n best and set the max length value to improve the quality of answer'''\n",
    "\n",
    "\"\"\"the code is modified based on the code from the following link, https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt#postprocessing\n",
    "turn discrete code into a function to simply the running.\"\"\"\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "def predict(model_checkpoint, test_data):\n",
    "\n",
    "    trained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "    eval_set_for_model = test_data.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "    eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "    batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(**batch)\n",
    "\n",
    "    start_logits = outputs.start_logits.cpu().numpy()\n",
    "    end_logits = outputs.end_logits.cpu().numpy()\n",
    "\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(test_data):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    n_best = 20\n",
    "    max_answer_length = 30\n",
    "    predicted = {}\n",
    "\n",
    "    for example in rc_test:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = test_data[\"offset_mapping\"][feature_index]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append(\n",
    "                        {\n",
    "                            \"text\": context[offsets[start_index][0]: offsets[end_index][1]],\n",
    "                            \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "        predicted[example_id] = {\"answers\": {\"text\": [best_answer[\"text\"]]}, \"id\": example_id}\n",
    "\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "#model_checkpoint =\"deepset/roberta-large-squad2\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "#model =  DistilBertForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "model_checkpoint = \"./results\"\n",
    "\n",
    "predicted = []\n",
    "\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "for test_data in (rc_test, rc_final):\n",
    "       test_data = preprocess_validation_dataset(test_data, tokenizer)\n",
    "       predicted.append(predict(model_checkpoint, test_data))\n",
    "\n",
    "with open('predicted_answers_finetuing_1.json', 'w') as f:\n",
    "        json.dump(predicted[0], f)\n",
    "with open('predicted_answers_finetuing_2.json', 'w') as f:\n",
    "        json.dump(predicted[1], f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "781ef2da3e9d983ee82cdc5eb270354f9ddafae9147a3214ae59f008783fff8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
