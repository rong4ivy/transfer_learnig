{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set up our environment.\n",
    "\n",
    "This Jupyter Notebook needs the following packages installed:\n",
    "- [PyTorch](https://pytorch.org/get-started/locally/)\n",
    "- [transformers](https://huggingface.co/docs/transformers/installation)\n",
    "- [datasets](https://huggingface.co/docs/datasets/installation)\n",
    "- [opendelta](https://opendelta.readthedocs.io/en/latest/notes/installation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare environment in Colab\n",
    "!pip install torch\n",
    "!pip install transformers==4.28.0\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install torchtext==0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/Users/rongwang/Desktop/DL project/path/to/cache\n",
      "env: HF_MODULES_CACHE=/Users/rongwang/Desktop/DL project/path/to/cache\n",
      "env: HF_DATASETS_CACHE=/Users/rongwang/Desktop/DL project/path/to/cache\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set cache directory if desired\n",
    "# if you do not set a cache directory then default values are used (usually '~/.cache')\n",
    "import os\n",
    "CACHE_DIR=os.path.abspath(os.path.expanduser('path/to/cache')) # I donot understand this line\n",
    "%set_env TRANSFORMERS_CACHE $CACHE_DIR\n",
    "%set_env HF_MODULES_CACHE $CACHE_DIR\n",
    "%set_env HF_DATASETS_CACHE $CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "## and then move your model and data to the device before you train or eval. Have fun folks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    DefaultDataCollator,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    DistilBertForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this if using Colab\n",
    "# load dataset\n",
    "# load dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "# The datasets library is a library for loading and preprocessing datasets for machine learning. The Dataset class is a way to handle large datasets in a way that is memory efficient.\n",
    "\n",
    "\n",
    "\n",
    "def read_data(filepath: str):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return Dataset.from_list(list(json.load(f).values()))\n",
    "\n",
    "\n",
    "filepath_1 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/squad_train.json')\n",
    "filepath_2 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/squad_dev.json')\n",
    "filepath_3 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/rc_train.json')\n",
    "filepath_4 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/rc_dev.json')\n",
    "\n",
    "filepath_5 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/rc_test_1.json')\n",
    "\n",
    "filepath_6 = os.path.abspath ('/content/gdrive/My Drive/data/DL_project/rc_test_1.json')\n",
    "\n",
    "squad_train = read_data(filepath_1)\n",
    "squad_dev = read_data(filepath_2)\n",
    "rc_train = read_data(filepath_3)\n",
    "rc_dev = read_data(filepath_4)\n",
    "rc_test = read_data(filepath_5)\n",
    "\n",
    "print(squad_train)\n",
    "print(rc_train)\n",
    "print(rc_dev)\n",
    "print(rc_test)\n",
    "print(rc_test[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context', 'question', 'id'],\n",
      "    num_rows: 177\n",
      "})\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Index 1126 out of range for dataset of size 1126.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m rc_final \u001b[39m=\u001b[39m read_data(filepath_6)\n\u001b[1;32m     36\u001b[0m \u001b[39mprint\u001b[39m(rc_test)\n\u001b[0;32m---> 37\u001b[0m small\u001b[39m=\u001b[39m rc_final\u001b[39m.\u001b[39;49mselect(\u001b[39mrange\u001b[39;49m(\u001b[39m1120\u001b[39;49m,\u001b[39m1127\u001b[39;49m))\n\u001b[1;32m     38\u001b[0m \u001b[39mprint\u001b[39m(small)\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    541\u001b[0m }\n\u001b[1;32m    542\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    544\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    545\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/datasets/fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/datasets/arrow_dataset.py:3771\u001b[0m, in \u001b[0;36mDataset.select\u001b[0;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[1;32m   3769\u001b[0m     \u001b[39mif\u001b[39;00m _is_range_contiguous(indices) \u001b[39mand\u001b[39;00m indices\u001b[39m.\u001b[39mstart \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3770\u001b[0m         start, length \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mstart, indices\u001b[39m.\u001b[39mstop \u001b[39m-\u001b[39m indices\u001b[39m.\u001b[39mstart\n\u001b[0;32m-> 3771\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select_contiguous(start, length, new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint)\n\u001b[1;32m   3772\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3773\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    541\u001b[0m }\n\u001b[1;32m    542\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    544\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    545\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/datasets/fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/datasets/arrow_dataset.py:3832\u001b[0m, in \u001b[0;36mDataset._select_contiguous\u001b[0;34m(self, start, length, new_fingerprint)\u001b[0m\n\u001b[1;32m   3829\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m   3831\u001b[0m _check_valid_indices_value(start, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m))\n\u001b[0;32m-> 3832\u001b[0m _check_valid_indices_value(start \u001b[39m+\u001b[39;49m length \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m))\n\u001b[1;32m   3833\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m length \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3834\u001b[0m     \u001b[39mreturn\u001b[39;00m Dataset(\n\u001b[1;32m   3835\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mslice(start, length),\n\u001b[1;32m   3836\u001b[0m         info\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mcopy(),\n\u001b[1;32m   3837\u001b[0m         split\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit,\n\u001b[1;32m   3838\u001b[0m         fingerprint\u001b[39m=\u001b[39mnew_fingerprint,\n\u001b[1;32m   3839\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/datasets/arrow_dataset.py:635\u001b[0m, in \u001b[0;36m_check_valid_indices_value\u001b[0;34m(index, size)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_valid_indices_value\u001b[39m(index, size):\n\u001b[1;32m    634\u001b[0m     \u001b[39mif\u001b[39;00m (index \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m index \u001b[39m+\u001b[39m size \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m size):\n\u001b[0;32m--> 635\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndex \u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m out of range for dataset of size \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Index 1126 out of range for dataset of size 1126."
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "# load dataset\n",
    "# prepare environment in Colab\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# The datasets library is a library for loading and preprocessing datasets for machine learning. The Dataset class is a way to handle large datasets in a way that is memory efficient.\n",
    "    \n",
    "\n",
    "def read_data(filepath: str):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return Dataset.from_list(list(json.load(f).values()))\n",
    "\n",
    "\n",
    "filepath_1 = os.path.abspath('rc_traindev/squad_train.json')\n",
    "filepath_2 = os.path.abspath('rc_traindev/squad_dev.json')\n",
    "filepath_3 = os.path.abspath('rc_traindev/rc_train.json')\n",
    "filepath_4 = os.path.abspath('rc_traindev/rc_dev.json')\n",
    "\n",
    "filepath_5 = os.path.abspath('rc_traindev/rc_test_1.json')\n",
    "filepath_6 = os.path.abspath('rc_traindev/rc_test_2.json')\n",
    "\n",
    "squad_train = read_data(filepath_1)\n",
    "squad_dev = read_data(filepath_2)\n",
    "rc_train = read_data(filepath_3)\n",
    "rc_dev = read_data(filepath_4)\n",
    "rc_test = read_data(filepath_5)\n",
    "rc_final = read_data(filepath_6)\n",
    "\n",
    "\n",
    "\n",
    "print(rc_test)\n",
    "small= rc_final.select(range(1120,1126))\n",
    "print(small)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will preprocess the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(dataset: Dataset, tokenizer, max_length: int = None):\n",
    "    \n",
    "    # dataset preprocessing function which can be used with datasets.map\n",
    "    # modified from https://huggingface.co/docs/transformers/tasks/question_answering\n",
    "    \n",
    "    # this function takes examples and extracts span start and span end on the token level as labels,\n",
    "    # contexts are truncated to fit into the model and samples are padded to the max_length,\n",
    "    # and information needed for evaluation is extracted\n",
    "    \n",
    "    def preprocess_function(examples, tokenizer, max_length):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        inputs = tokenizer(\n",
    "            questions,\n",
    "            examples[\"context\"],\n",
    "            max_length=max_length,\n",
    "            truncation=\"only_second\",\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        offset_mapping = inputs[\"offset_mapping\"]\n",
    "        answers = examples[\"answers\"]\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        context_spans = []\n",
    "\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            answer = answers[i]\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "            context_spans.append((context_start, context_end))\n",
    "\n",
    "            # If the answer is not fully inside the context, label it (0, 0)\n",
    "            if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Otherwise it's the start and end token positions\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "        inputs[\"start_positions\"] = start_positions\n",
    "        inputs[\"end_positions\"] = end_positions\n",
    "        inputs[\"context_span\"] = context_spans\n",
    "        return inputs\n",
    "\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        fn_kwargs=dict(tokenizer=tokenizer, max_length=max_length),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc770830521408db201e1b3b3e03144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b55da9f1e034d1fb2b6e4547b0ae0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115a43ed36cd494a8709ec2dcfbbe881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3906460b1f4ac4846e281df690fc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess data\n",
    "\n",
    "max_length = 500 ## max length of context + question in tokens, maybe it was too expensive for the GPU \n",
    "\n",
    "model_checkpoint =\"deepset/roberta-large-squad2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenized_squad_train = preprocess_dataset(squad_train, tokenizer)\n",
    "\n",
    "tokenized_squad_dev = preprocess_dataset(squad_dev, tokenizer)\n",
    "\n",
    "tokenized_rc_train = preprocess_dataset(rc_train, tokenizer)\n",
    "tokenized_rc_dev = preprocess_dataset(rc_dev, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn(dataset: Dataset, context_spans, offset_mappings):\n",
    "    # set up metric and labels\n",
    "    squad_metric = load_metric(\"squad\")\n",
    "    \n",
    "    #Handle missing keys: If some samples might not have an 'answers' key, you can use the dict.get() method, which returns None if the key is not present in the dictionary.\n",
    "    # Here's how you can modify your code:\n",
    "    references = [{\"answers\": sample.get(\"answers\"), \"id\": sample.get(\"id\")} for sample in dataset]\n",
    "\n",
    "\n",
    "    # this function extracts the span answers and computes the f1 score\n",
    "    # note that we use an approximation for extracting the best span, i.e. we do not consider all possibilities here\n",
    "    def evaluate(predictions: EvalPrediction):\n",
    "        preds_start_probs = predictions.predictions[0]\n",
    "        preds_end_probs = predictions.predictions[1]\n",
    "        preds_start_idx = preds_start_probs.argmax(axis=1)\n",
    "        preds_end_idx = preds_end_probs.argmax(axis=1)\n",
    "        predictions = [\n",
    "            {\n",
    "                \"prediction_text\": context[offset_mapping[pred_start_idx][0] : offset_mapping[pred_end_idx][1] + 1]\n",
    "                if context_start_idx <= pred_start_idx <= context_end_idx\n",
    "                and context_start_idx <= pred_end_idx <= context_end_idx\n",
    "                else \"\",\n",
    "                \"id\": sample_id,\n",
    "            }\n",
    "            for sample_id, context, (\n",
    "                context_start_idx,\n",
    "                context_end_idx,\n",
    "            ), offset_mapping, pred_start_idx, pred_end_idx in zip(\n",
    "                dataset[\"id\"], dataset[\"context\"], context_spans, offset_mappings, preds_start_idx, preds_end_idx\n",
    "            )\n",
    "        ]\n",
    "        return squad_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "## and then move your model and data to the device before you train or eval. Have fun folks!\n",
    "\n",
    "#installing pytorch to run on mac M1, using \"mps\" instead of 'cuda'.\n",
    "\n",
    "torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/pvlsm7s116zd61_5k5qx9hkr0000gn/T/ipykernel_7968/2530109866.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  squad_metric = load_metric(\"squad\")\n"
     ]
    }
   ],
   "source": [
    "# set up training arguments and pass to trainer, For few-shot learning, I only trained on rc_train and rc_dev, altogether 200 samples, use 5 epochs \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",    # if running on Colab, set this to \"/content/results\"\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=32,\n",
    "    num_train_epochs=5,  # max_steps will override this value\n",
    "    # max_steps=1000,  # comment out if this is not wanted\n",
    "    weight_decay=0.01,\n",
    "    #logging_dir='./logs',\n",
    "    label_names=[\"start_positions\", \"end_positions\"]\n",
    ")\n",
    "\n",
    "# data collator for batching\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# the actual trainer which performs training and evaluation\n",
    "trainer = Trainer(\n",
    "    model= model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_rc_train,\n",
    "    eval_dataset=tokenized_rc_dev,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=get_evaluate_fn(\n",
    "        rc_dev,\n",
    "        tokenized_rc_dev[\"context_span\"],\n",
    "        tokenized_rc_dev[\"offset_mapping\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do an initial evaluation\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform training\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "\n",
    "max_length = 384 ## max length of context + question in tokens, maybe it was too expensive for the GPU \n",
    "\n",
    "model_checkpoint =\"deepset/roberta-large-squad2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_dataset(dataset: Dataset, tokenizer, max_length: int = 500, stride: int =250 ):\n",
    "\n",
    "    def preprocess_function(examples, tokenizer, max_length):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        inputs = tokenizer(\n",
    "            questions,\n",
    "            examples[\"context\"],\n",
    "            max_length=max_length,\n",
    "            truncation=\"only_second\",\n",
    "            stride=stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        # Create a map from overflowing tokens to the original sample they came from\n",
    "        sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "        inputs[\"example_id\"] = [examples[\"id\"][i] for i in sample_map]\n",
    "\n",
    "        # Modify the offset mapping to only keep offsets for the context\n",
    "        for i in range(len(inputs[\"input_ids\"])):\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "            offset = inputs[\"offset_mapping\"][i]\n",
    "            inputs[\"offset_mapping\"][i] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        fn_kwargs=dict(tokenizer=tokenizer, max_length=max_length),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the following code is for the prediction part for test data, without any answers labels.\n",
    "compared with the prediction part for train and validation data, we don't need to compute the f1 score.\n",
    "but it is more complicated, it chohoses the n best and set the max length value to improve the quality of answer'''\n",
    "\n",
    "\"\"\"the code is modified based on the code from the following link, https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt#postprocessing\n",
    "turn discrete code into a function to simply the running.\"\"\"\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "def predict(model_checkpoint, test_data):\n",
    "\n",
    "    trained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "    eval_set_for_model = test_data.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "    eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "    batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(**batch)\n",
    "\n",
    "    start_logits = outputs.start_logits.cpu().numpy()\n",
    "    end_logits = outputs.end_logits.cpu().numpy()\n",
    "\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(test_data):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    n_best = 20\n",
    "    max_answer_length = 30\n",
    "    predicted = {}\n",
    "\n",
    "    for example in rc_test:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = test_data[\"offset_mapping\"][feature_index]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append(\n",
    "                        {\n",
    "                            \"text\": context[offsets[start_index][0]: offsets[end_index][1]],\n",
    "                            \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "        predicted[example_id] = {\"answers\": {\"text\": [best_answer[\"text\"]]}, \"id\": example_id}\n",
    "\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b268084f54a14b66a185ef76152f290d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/177 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess data\n",
    "\n",
    "max_length = 500 ## max length of context + question in tokens, maybe it was too expensive for the GPU \n",
    "\n",
    "model_checkpoint =\"deepset/roberta-large-squad2\"\n",
    "\n",
    "test_data_1 = preprocess_validation_dataset(rc_test, tokenizer)\n",
    "result_1 = predict(model_checkpoint, test_data_1)\n",
    "\n",
    "with open('predicted_baseline_1.json', 'w') as f:\n",
    "        json.dump(result_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the following code is for the prediction part for test data, without any answers labels.\n",
    "compared with the prediction part for train and validation data, we don't need to compute the f1 score.\n",
    "but it is more complicated, it chohoses the n best and set the max length value to improve the quality of answer'''\n",
    "\n",
    "\"\"\"the code is modified based on the code from the following link, https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt#postprocessing\n",
    "turn discrete code into a function to simply the running.\"\"\"\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "def predict(model_checkpoint, test_data):\n",
    "\n",
    "    trained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "    eval_set_for_model = test_data.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "    eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "    batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(**batch)\n",
    "\n",
    "    start_logits = outputs.start_logits.cpu().numpy()\n",
    "    end_logits = outputs.end_logits.cpu().numpy()\n",
    "\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(test_data):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    n_best = 20\n",
    "    max_answer_length = 30\n",
    "    predicted = {}\n",
    "\n",
    "    for example in rc_final:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = test_data[\"offset_mapping\"][feature_index]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append(\n",
    "                        {\n",
    "                            \"text\": context[offsets[start_index][0]: offsets[end_index][1]],\n",
    "                            \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "        predicted[example_id] = {\"answers\": {\"text\": [best_answer[\"text\"]]}, \"id\": example_id}\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_checkpoint \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeepset/roberta-large-squad2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m test_data_1 \u001b[39m=\u001b[39m preprocess_validation_dataset(rc_test, tokenizer)\n\u001b[1;32m      4\u001b[0m result_1 \u001b[39m=\u001b[39m predict(model_checkpoint, test_data_1)\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpredicted_baseline.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model_checkpoint =\"deepset/roberta-large-squad2\"\n",
    "\n",
    "test_data_1 = preprocess_validation_dataset(rc_final, tokenizer)\n",
    "result_1 = predict(model_checkpoint, test_data_1)\n",
    "\n",
    "with open('predicted_baseline.json', 'w') as f:\n",
    "        json.dump(result_1, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "781ef2da3e9d983ee82cdc5eb270354f9ddafae9147a3214ae59f008783fff8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
